{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lgtm53dkxqES"
      },
      "source": [
        "# Text classification using BERT on Clothing Data.\n",
        "\n",
        "**This guide has been adapted and modified from [huggingface](https://huggingface.co/docs/transformers/en/tasks/sequence_classification).**\n",
        "\n",
        "This guide will show you how to:\n",
        "\n",
        "1. Finetune [DistilBERT](https://huggingface.co/distilbert-base-uncased) a custom dataset.\n",
        "2. Use your finetuned model for inference.\n",
        "\n",
        "\n",
        "Before you begin, make sure you have all the necessary libraries installed:\n",
        "\n",
        "```bash\n",
        "pip install transformers datasets evaluate pandas torch\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRrEICvGxqEV"
      },
      "source": [
        "## Load the dataset from csv file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ISS-User1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "df = pd.read_csv(\n",
        "    r'C:\\Users\\ISS-User1\\Desktop\\eugene\\Glowing-Torch\\Datasets\\dataset.csv', encoding='latin1')\n",
        "hf_dataset = Dataset.from_pandas(df)\n",
        "hf_dataset = hf_dataset.train_test_split(\n",
        "    test_size=0.8, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxKvBP0cxqEX"
      },
      "source": [
        "Collect the classification labels from the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OKJCQx4wxqEY"
      },
      "outputs": [],
      "source": [
        "x_train = hf_dataset['train']['name']\n",
        "x_test = hf_dataset['test']['name']\n",
        "y_train = hf_dataset['train']['category']\n",
        "y_test = hf_dataset['test']['category']\n",
        "labels = set()\n",
        "for label in y_test:\n",
        "    labels.add(label)\n",
        "for label in y_train:\n",
        "    labels.add(label)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will now represent each label with their respective ids and create a mapping of label to id and vice versa from all labels of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "id2label = {}\n",
        "label2id = {}\n",
        "for i, label in enumerate(labels):\n",
        "    id2label[i] = label\n",
        "    label2id[label] = i\n",
        "label_dict = {\"id2label\": id2label, \"label2id\": label2id}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After creation of the label mappings, we will now update the text labels into their id labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "for i, _ in enumerate(y_train):\n",
        "    y_train[i] = label2id[y_train[i]]\n",
        "for i, _ in enumerate(y_test):\n",
        "    y_test[i] = label2id[y_test[i]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will now create a new HuggingFace `DatasetDict` to store our newly map labels with its respective product names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_test = hf_dataset['test']['name']\n",
        "x_train = hf_dataset['train']['name']\n",
        "dataset = DatasetDict({'train': Dataset.from_dict({'label': y_train, 'text': x_train}),\n",
        "                       'test': Dataset.from_dict({'label': y_test, 'text': x_test})})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can visualize the dataset below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['label', 'text'],\n",
            "        num_rows: 6338\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['label', 'text'],\n",
            "        num_rows: 25353\n",
            "    })\n",
            "})\n",
            "{'label': 6, 'text': 'Twill chinos'}\n"
          ]
        }
      ],
      "source": [
        "print(dataset)\n",
        "print(dataset[\"train\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TSKjw2QxqEa"
      },
      "source": [
        "There are two fields in this dataset:\n",
        "\n",
        "- `text`: the movie review text.\n",
        "- `label`: a value ranging from a `0` to `len(labels)` which represent a clothing category"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHjTQ6dCxqEa"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zBu_o29xqEa"
      },
      "source": [
        "The next step is to load a DistilBERT tokenizer to preprocess the `text` field:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2sfZwhmkxqEb"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFcsocpGxqEb"
      },
      "source": [
        "Create a preprocessing function to tokenize `text` and truncate sequences to be no longer than DistilBERT's maximum input length:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ab8Ht-t_xqEb"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIO96uazxqEb"
      },
      "source": [
        "To apply the preprocessing function over the entire dataset, use ðŸ¤— Datasets [map](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map) function. You can speed up `map` by setting `batched=True` to process multiple elements of the dataset at once:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "X2ShmQCJxqEc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6338/6338 [00:00<00:00, 41770.96 examples/s]\n",
            "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25353/25353 [00:00<00:00, 36118.19 examples/s]\n"
          ]
        }
      ],
      "source": [
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tV2xpmPixqEc"
      },
      "source": [
        "Now create a batch of examples using [DataCollatorWithPadding](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorWithPadding). It's more efficient to *dynamically pad* the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "81roR2lOxqEc"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiVm1L75xqEd"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bg22_0gFxqEd"
      },
      "source": [
        "Including a metric during training is often helpful for evaluating your model's performance. You can quickly load a evaluation method with the ðŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index) library. For this task, load the [accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy) metric (see the ðŸ¤— Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour) to learn more about how to load and compute a metric):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qsFddjamxqEd"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjKtiPEWxqEd"
      },
      "source": [
        "Then create a function that passes your predictions and labels to [compute](https://huggingface.co/docs/evaluate/main/en/package_reference/main_classes#evaluate.EvaluationModule.compute) to calculate the accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uiXm4gt7xqEd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMUASaq2xqEe"
      },
      "source": [
        "Your `compute_metrics` function is ready to go now, and you'll return to it when you setup your training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCjfk_eGxqEe"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6K4sR76xqEf"
      },
      "source": [
        "<Tip>\n",
        "\n",
        "If you aren't familiar with finetuning a model with the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer), take a look at the basic tutorial [here](https://huggingface.co/docs/transformers/main/en/tasks/../training#train-with-pytorch-trainer)!\n",
        "\n",
        "</Tip>\n",
        "\n",
        "You're ready to start training your model now! Load DistilBERT with [AutoModelForSequenceClassification](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForSequenceClassification) along with the number of expected labels, and the label mappings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zKF9Hls9xqEf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\", num_labels=len(labels), id2label=id2label, label2id=label2id\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we \"move\" the model to the GPU.\n",
        "The architecture of the model can be also visualised from the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DistilBertForSequenceClassification(\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will use a subset of the dataset to reduce compuation time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "small_train_dataset = tokenized_dataset[\"train\"].shuffle(\n",
        "    seed=42) .select(range(100))\n",
        "small_eval_dataset = tokenized_dataset[\"test\"].shuffle(\n",
        "    seed=42) .select(range(100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMeytScvxqEf"
      },
      "source": [
        "At this point, only three steps remain:\n",
        "\n",
        "1. Define your training hyperparameters in [TrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments)\n",
        "2. At the end of each epoch, the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) will evaluate the accuracy and save the training checkpoint.\n",
        "3. Pass the training arguments to [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) along with the model, dataset, tokenizer, data collator, and `compute_metrics` function.\n",
        "4. Call [train()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train) to finetune your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ZvYiul8pxqEf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 29%|â–ˆâ–ˆâ–Š       | 2/7 [00:02<00:06,  1.28s/it]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                             \n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:09<00:00,  1.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 2.1764607429504395, 'eval_accuracy': 0.35, 'eval_runtime': 0.8548, 'eval_samples_per_second': 116.993, 'eval_steps_per_second': 8.19, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:10<00:00,  1.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 10.8156, 'train_samples_per_second': 9.246, 'train_steps_per_second': 0.647, 'train_loss': 2.241724286760603, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=7, training_loss=2.241724286760603, metrics={'train_runtime': 10.8156, 'train_samples_per_second': 9.246, 'train_steps_per_second': 0.647, 'train_loss': 2.241724286760603, 'epoch': 1.0})"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"my_awesome_model\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(training_args.device) # see where is the model being trained\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2s7hveRqxqEg"
      },
      "source": [
        "<Tip>\n",
        "\n",
        "[Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) applies dynamic padding by default when you pass `tokenizer` to it. In this case, you don't need to specify a data collator explicitly.\n",
        "\n",
        "</Tip>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87Yu0yvzxqEg"
      },
      "source": [
        "<Tip>\n",
        "\n",
        "For a more in-depth example of how to finetune a model for text classification, take a look at the corresponding\n",
        "[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb)\n",
        "or [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb).\n",
        "\n",
        "</Tip>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference\n",
        "\n",
        "We will now try to make a prediction with our fine-tuned model of BERT.\n",
        "We can set a sample clothing name to predict it's category and use the tokenizer to encode it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "name = \"Cotton short-sleeve shirt\"\n",
        "inputs = tokenizer.encode(name, return_tensors=\"pt\").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will now make a prediction with the tokenized input.\n",
        "The logits represent the unnormalized probabilites from the output of the neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.0676, -0.1646, -0.0449, -0.1894,  0.1228, -0.0996,  0.1313,  0.4119,\n",
            "         -0.0779, -0.0822]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "logits = model(inputs).logits\n",
        "print(logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use `torch.max()` to obtain the index of the label with the highest probability.\n",
        "Using the index and the previously obtained id2label map, we can get the category predicted for the product item."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted: tops for Cotton short-sleeve shirt\n"
          ]
        }
      ],
      "source": [
        "predictions = torch.max(logits, 1).indices\n",
        "prediction: str = id2label[predictions.tolist()[0]]\n",
        "print(f\"Predicted: {prediction} for {name}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
